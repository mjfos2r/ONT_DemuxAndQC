version 1.0

import "../tasks/QC/NanoPlot.wdl" as NP
import "../tasks/utilities/GeneralUtils.wdl" as GenUtils
import "../tasks/utilities/SamplesheetUtils.wdl" as SSUtils

workflow ONT_DemuxAndQC {
    meta {
        description: "Take in the tarball of the bam_pass file for our run, decompress it, merge all of the bams for each barcode, rename the barcode, then trim, filter, and generate QC report for all of our samples. Also generate a NanoPlot from the ONT summary.txt file"
        author: "Michael J. Foster"
        version: "v2.10.0"
    }

    parameter_meta {
        RunTarball: "Gzipped tarball of bam_pass generated by the sequencer"
        RunChecksum: "md5sum of the tarball containing all of the directories"
        summary_files: "sequencing summary file(s) generated by dorado"
        summary_checksums: "checksum file(s) for our sequencing summary file(s) generated by dorado"
        samplesheet: "samplesheet containing sample information, most importantly, the barcode_id and sample_alias"
        singleplex: "boolean flag to indicate whether the run being processed is barcoded or singleplex. must specify sample_id if set to true (default: false)"
        sample_id: "optional string to pass if the run being decompressed is a singleplex run."
    }

    input {
        String TargetDataTableID
        File samplesheet
        File RunTarball
        File RunChecksum
        Array[File] summary_files
        Array[File] summary_checksums
        File? raw_hash_file
        File? raw_hash_digest
        Boolean singleplex = false
        String? sample_id
    }
    ## All of the md5sum validation needs to be migrated over to terra's gcpmd5sum task.
    # not breaking everything right after it's functional. that's a downstream problem.

    # scatter across the summary file(s) and pull the name(s) for our validation array # this needs fixing.
    scatter (file in summary_files){
        String summary_filename = basename(file)
    }
    Array[String] summary_filenames = summary_filename

    # now we validate our summary file(s)
    scatter (idx in range(length(summary_files))) {
        call GenUtils.ValidateMd5sum as summary_validation {
            input:
                file = summary_files[idx],
                checksum = summary_checksums[idx]
        }
        Boolean summary_is_valid = read_boolean(summary_validation.is_valid)
    }

    # gather our validation statuses. it'll be useful when it fails due to corruption that can't be dealt with in a conditional check (for some reason).
    Array[Boolean] summary_validity = summary_is_valid
    Array[Pair[String, Boolean]] summary_integrity = zip(summary_filenames, summary_validity)

    # begone validation checks. If it's invalid just check after the execution fails...
    call NP.NanoPlotFromSummary { input: summary_files = summary_files, is_valid = summary_is_valid }

    # input this to the decompression step so that we can stage this and prevent issues with cromwell trying to move the same file at the same time.
    # Since I think that may be an issue. (I have no proof just a hunch)
    call GenUtils.DecompressRunTarball {
        input:
            tarball = RunTarball,
            tarball_hash = RunChecksum,
            raw_hash_file = raw_hash_file,
            raw_hash_digest = raw_hash_digest,
            singleplex = singleplex,
            sample_id = sample_id
    }

    # and now we parse our samplesheet using the paths from above!
    # this will either fail or work perfectly.
    call SSUtils.ParseSamplesheetToDataTable {
        input:
            samplesheet = samplesheet,
            file_paths = DecompressRunTarball.glob_paths,
            TargetDataTableID = TargetDataTableID
    }

    output {
        # Metadata parsed from samplesheet
        File samplesheet_with_reads = ParseSamplesheetToDataTable.samplesheet_with_reads
        File samplesheet_with_reads_json = ParseSamplesheetToDataTable.samplesheet_with_reads_json

        # Validation output
        Array[Pair[String, Boolean]] summary_file_integrity = summary_integrity
        Boolean run_validation_status = DecompressRunTarball.is_valid
        File corrupted_files = DecompressRunTarball.corrupted_files

        # decompressed outputs from DecompressRunTarball if md5 is valid.
        Int directory_count = DecompressRunTarball.directory_count
        Array[Int] file_counts = DecompressRunTarball.file_counts
        Array[String] barcode = DecompressRunTarball.barcode
        Array[File] merged_reads = DecompressRunTarball.merged_reads
        File merged_reads_gcp_paths = DecompressRunTarball.glob_paths
        Array[File] file_list = DecompressRunTarball.file_list

        # NanoPlot outputs
        File nanoplot_map = NanoPlotFromSummary.map
        File  nanoplot_tarball = NanoPlotFromSummary.tarball
        Map[String, Float]  nanoplot_stats_map = NanoPlotFromSummary.stats_map
    }
}
